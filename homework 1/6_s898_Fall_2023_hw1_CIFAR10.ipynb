{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingpengHong/6.s898-deep-learning/blob/main/6_s898_Fall_2023_hw1_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp7RKvwbwbpa"
      },
      "source": [
        "**NOTE:** For all questions in this notebook, include your answers in the pdf submission. Do **not** submit the colab.\n",
        "\n",
        "<h1>CIFAR-10</h1>\n",
        "\n",
        "CIFAR-10 is a image classification dataset.\n",
        "+ Each data sample is an RGB $32\\times32$ real image. A raw loaded image $\\in \\mathbb{R}^{3 \\times 32 \\times 32}$.\n",
        "+ Each image is associated with a label $\\in [0,1 ,2 ,3 ,4, 5, 6, 7, 8, 9]$.\n",
        "\n",
        "<table>\n",
        "    <tbody><tr>\n",
        "        <td class=\"cifar-class-name\">airplane</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">automobile</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">bird</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">cat</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">deer</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">dog</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">frog</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">horse</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">ship</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">truck</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "</tbody></table>\n",
        "(Table credit to Alex Krizhevsky's [webpage](https://www.cs.toronto.edu/~kriz/cifar.html).)\n",
        "\n",
        "Our goal is to train a neural network classifier that takes such $3\\times28\\times28$ images and predict a label $\\in \\{0, 1, 2, \\dots, 9\\}$.\n",
        "\n",
        "You will first implement a neural network with manual backpropagation rules, and then train it on CIFAR-10 classification.\n",
        "\n",
        "**NOTE:** A lot of skeleton code is provided to you already. Make sure to read through and understand them, as less such code will be provided as we are further into the course and more used to deep learning code structure. **Unless asked to, please do not modify provided code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MEnmRQuzLFj"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alPW6hlwE71a"
      },
      "source": [
        "You should run on GPU-enabled colab server (should be default for this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYaubOb0E6u7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "assert torch.cuda.is_available(), \"Should use GPU-enabled colab\"\n",
        "\n",
        "device = torch.device('cuda:0')  # we will train with CUDA!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6SOqXmUzLg9"
      },
      "source": [
        "## Modules\n",
        "\n",
        "In this notebook, we will use a our own `Module` classes, which has a similar API as PyTorch's `nn.Module`, except that we will manually implement our own backprop operations (`module.backward`) rather than using PyTorch's `autograd`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fsMeIjy2T2"
      },
      "source": [
        "Read through the method definitions on `Module` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPEYe7bbwbpd"
      },
      "outputs": [],
      "source": [
        "from typing import *\n",
        "import abc\n",
        "\n",
        "\n",
        "# This is our Module API\n",
        "class Module(abc.ABC):\n",
        "    device: Optional[torch.device]  # Parameters should live on this device!\n",
        "    inputs: Tuple[torch.Tensor, ...]\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def parameters(self) -> Iterator[torch.Tensor]:\n",
        "        r'''\n",
        "        Returns an iterator over the *parameters* of this module.\n",
        "\n",
        "        Subclass needs to implement this.\n",
        "        '''\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, *inputs: torch.Tensor) -> torch.Tensor:\n",
        "        r'''\n",
        "        Returns the output of applying this module on tensors `inputs`, each of\n",
        "        which is a *batched* tensor.\n",
        "\n",
        "        In most cases, the module takes a single input tensor (e.g., linear\n",
        "        layer and ReLU layers). However, multiple inputs are useful when the\n",
        "        module computes a loss between a prediction and groundtruth target.\n",
        "\n",
        "        Subclass needs to implement this.\n",
        "        '''\n",
        "\n",
        "    def __call__(self, *inputs: torch.Tensor) -> torch.Tensor:\n",
        "        r'''\n",
        "        Simply calls forward, and stores inputs at `self.inputs`, which may be\n",
        "        useful for computing gradients in `backward`.\n",
        "        '''\n",
        "        self.inputs = inputs\n",
        "        return self.forward(*inputs)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def backward(self, dLdout: torch.Tensor) -> torch.Tensor:\n",
        "        r'''\n",
        "        This is our manual backprop.\n",
        "\n",
        "        Given, `dLdOut` as $dL / d output$, for some loss `L`, we compute\n",
        "        1. For each parameter `p` of this module, compute $d L /d p$, stored at `p.grad`.\n",
        "        2. $dL / d self.inputs[0]$, to be passed to the previous layer. Only\n",
        "           needs to compute derivative of the first input.\n",
        "\n",
        "        Note that $dL / d *$ should always be a tensor of same shape as *. E.g.,\n",
        "        $d L /d p$ (i.e., `p.grad`) should always be of the same shape as `p`.\n",
        "\n",
        "        Subclass needs to implement this.\n",
        "        '''\n",
        "\n",
        "    def zero_grad(self):\n",
        "        r'''\n",
        "        Clear any previous computed gradients.\n",
        "        '''\n",
        "        for p in self.parameters():\n",
        "            p.grad = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INlLlaK82RQZ"
      },
      "source": [
        "We will use three kinds of modules to compose our network:\n",
        "+ Linear layer (also called fully-connected layer)\n",
        "+ ReLU activation (our non-linearity)\n",
        "+ Cross entropy loss (our classification loss)\n",
        "\n",
        "**Question 1**: Complete the incomplete `forward` and `backward` definitions of the following module classes, each using `<=5` lines of code. They are marked with `FIXME`. Add your code to the submission pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JyhOIJJ1cug"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    def __init__(self, in_features: int, out_features: int, device=None):\n",
        "        super().__init__(device)\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = torch.randn(out_features, in_features, device=device) / in_features  # weight matrix\n",
        "        self.bias = torch.zeros(out_features, device=device)  # bias vector\n",
        "\n",
        "    def parameters(self) -> Iterator[torch.Tensor]:\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x           shape: [b, in_features]\n",
        "        # self.weight shape: [out_features, in_features]\n",
        "        # self.bias   shape: [out_features]\n",
        "        #\n",
        "        # output should have shape: [b, out_features]\n",
        "        #\n",
        "        # FIXME\n",
        "        pass\n",
        "\n",
        "    def backward(self, dLdout: torch.Tensor) -> torch.Tensor:\n",
        "        # self.inputs[0] shape: [b, in_features]\n",
        "        # dLdout         shape: [b, out_features]\n",
        "        # self.weight    shape: [out_features, in_features]\n",
        "        # self.bias      shape: [out_featurs]\n",
        "        #\n",
        "        # FIXME\n",
        "        # Note that you should *not* modify `dLdout` or `self.inputs` inplace.\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Linear(in={self.in_features}, out={self.out_features})\"\n",
        "\n",
        "\n",
        "class ReLU(Module):\n",
        "    def parameters(self) -> Iterator[torch.Tensor]:\n",
        "        return []\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x.clamp(min=0)\n",
        "\n",
        "    def backward(self, dLdout: torch.Tensor) -> torch.Tensor:\n",
        "        # self.inputs[0]  shape: [b, d]\n",
        "        # dLdout          shape: [b, d]\n",
        "        #\n",
        "        # FIXME\n",
        "        # Note that you should *not* modify `dLdout` or `self.inputs` inplace.\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"ReLU()\"\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(Module):\n",
        "    def parameters(self) -> Iterator[torch.Tensor]:\n",
        "        return []\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        # logits    shape: [b, num_classes]\n",
        "        # target    shape: [b], containing *integers* in [0, 1, ..., num_classes - 1]\n",
        "        #\n",
        "        # For each logits in the batch\n",
        "        #   p = softmax(logits)\n",
        "        #   loss = -log(p[target])\n",
        "        # Total loss is averaged across the entire batch.\n",
        "        b = logits.shape[0]\n",
        "        return -logits.softmax(dim=-1).log()[torch.arange(b, device=self.device), target].mean()  # scalar, shape: []\n",
        "\n",
        "    def backward(self, dLdout: torch.Tensor) -> torch.Tensor:\n",
        "        logits, target = self.inputs\n",
        "        # logits    shape: [b, num_classes]\n",
        "        # target    shape: [b], containing *integers* in [0, 1, ..., num_classes - 1]\n",
        "        # dLdout    shape: []\n",
        "        #\n",
        "        # FIXME\n",
        "        # Note that you should *not* modify `dLdout` or `self.inputs` inplace.\n",
        "        # Compute dL / d logits\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"CrossEntropyLoss()\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHITDDZY2QGB"
      },
      "source": [
        "Now, create our network!\n",
        "\n",
        "It takes in `input_dim`-dimensional input, and output $10$-dimensional output, representing the logits of the predicted probabilities.\n",
        "\n",
        "We use a 3 layer ReLU network with width 8192."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pDtWqE98Br5"
      },
      "outputs": [],
      "source": [
        "class Network(Module):\n",
        "    layers: List[Module]\n",
        "\n",
        "    def __init__(self, input_dim, device=None):\n",
        "        super().__init__(device)\n",
        "        self.layers = [\n",
        "            Linear(input_dim, 8192, device=device),\n",
        "            ReLU(device=device),\n",
        "            Linear(8192, 8192, device=device),\n",
        "            ReLU(device=device),\n",
        "            Linear(8192, 10, device=device),\n",
        "        ]\n",
        "\n",
        "    def parameters(self):\n",
        "        r'''\n",
        "        Parameters are from layers.\n",
        "        '''\n",
        "        for layer in self.layers:\n",
        "            yield from layer.parameters()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        r'''\n",
        "        Sequentially activate all layers.\n",
        "        '''\n",
        "        y = x\n",
        "        for layer in self.layers:\n",
        "            y = layer(y)\n",
        "        return y\n",
        "\n",
        "    def backward(self, dLdout: torch.Tensor) -> torch.Tensor:\n",
        "        r'''\n",
        "        Backpropagation: backward through each layer in reverse order.\n",
        "        '''\n",
        "        for layer in self.layers[::-1]:\n",
        "            dLdout = layer.backward(dLdout)\n",
        "        return dLdout\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        repr_str = \"Network(\\n\"\n",
        "        for layer in self.layers:\n",
        "            repr_str += \"    \" + repr(layer) + \"\\n\"\n",
        "        repr_str += ')'\n",
        "        return repr_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwPYeNKNBYGa"
      },
      "source": [
        "Test your implementation by running the following!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUvCklzY_vE9"
      },
      "outputs": [],
      "source": [
        "model = Network(input_dim=64, device=device)\n",
        "input = torch.randn(2, 64, device=device)\n",
        "print(model)\n",
        "print('output=\\n', model(input))\n",
        "assert model.backward(torch.randn(2, 10, device=device)).shape == input.shape\n",
        "print('backward works!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyjHU6NdCEjT"
      },
      "source": [
        "You can also verify the computed gradients via finite difference. E.g.,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBAHplMKCpi9"
      },
      "outputs": [],
      "source": [
        "# Compute gradient for the first_linear.weight[45, 34] w.r.t. model(input).sum().\n",
        "\n",
        "model.zero_grad()  # clear up the previously computed gradients\n",
        "input = torch.randn(10, 64, device=device) * 10\n",
        "model.backward(torch.ones_like(model(input)))  # for sum, dLdout is all ones.\n",
        "backprop_grad = model.layers[0].weight.grad[845, 34]\n",
        "\n",
        "print('Our manual backprop grad is ', backprop_grad.item())\n",
        "\n",
        "eps = 1e-5\n",
        "# -eps\n",
        "model.layers[0].weight[845, 34] -= eps\n",
        "output_0 = model(input).sum()\n",
        "# +eps\n",
        "model.layers[0].weight[845, 34] += 2 * eps\n",
        "output_1 = model(input).sum()\n",
        "# numerical\n",
        "numerical_grad = (output_1 - output_0) / (2 * eps)\n",
        "\n",
        "print('Numerical backprop grad is ', numerical_grad.item())\n",
        "print('Any difference on the order of 1e-5 is fine.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68F-Ie39G4jS"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We will use the `torchvision.datasets.CIFAR10` class to load CIFAR10 dataset.\n",
        "\n",
        "By default, the loaded data are in `PIL.Image` format, which are perfect for visualization. Let's take a look!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfpG_Aq4HBqT"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def get_datasets(train_transforms=(), val_transforms=()):\n",
        "    r\"\"\"\n",
        "    Returns the CIFAR-10 training and validation datasets with corresponding\n",
        "    transforms.\n",
        "\n",
        "    `*_transforms` represent optional transformations, e.g., conversion to\n",
        "    PyTorch tensors, preprocessing, etc.\n",
        "    \"\"\"\n",
        "    train_set = torchvision.datasets.CIFAR10(\n",
        "        './data', train=True, download=True,\n",
        "        transform=torchvision.transforms.Compose(train_transforms))\n",
        "    val_set = torchvision.datasets.CIFAR10(\n",
        "        './data', train=False, download=True,\n",
        "        transform=torchvision.transforms.Compose(val_transforms))\n",
        "    return train_set, val_set\n",
        "\n",
        "\n",
        "train_set, val_set = get_datasets()\n",
        "\n",
        "print(f\"Training set size: {len(train_set)}\")\n",
        "print(f\"Validation set size: {len(val_set)}\")\n",
        "\n",
        "class_names = train_set.classes\n",
        "\n",
        "print(f'CIFAR-10 classes: {class_names}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiv-WnXkblOZ"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsCEwdjtHTze"
      },
      "outputs": [],
      "source": [
        "# Pick a training sample\n",
        "\n",
        "data, label = train_set[13]\n",
        "print('data has type:', type(data))\n",
        "print('data has label:', class_names[label])\n",
        "data.resize((200, 200), resample=Image.NEAREST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L30ugpKebnL2"
      },
      "source": [
        "### Tenosr Conversion and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96jgZCfFy7k5"
      },
      "source": [
        "For actual training, we will use a series of transforms to convert them into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ati9BLIaXQ"
      },
      "outputs": [],
      "source": [
        "data_transforms = [\n",
        "    # Converts PIL images into tensors with values in [0, 1].\n",
        "    # For RGB images, the output shape will be [3, width, height], where `3`\n",
        "    # represents three channels.\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    # Normalize each of the three channels so that they have 0 mean and 1 std.\n",
        "    # Normalization is generally a good idea and particularly useful when different\n",
        "    # parts of your input have very different statistics.\n",
        "    torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
        "]\n",
        "\n",
        "train_set, val_set = get_datasets(train_transforms=data_transforms, val_transforms=data_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RCDJPb2Rdrk"
      },
      "outputs": [],
      "source": [
        "# Now, the samples are tensors!\n",
        "data, label = train_set[13]\n",
        "print('data has type:', type(data))\n",
        "print('data has shape:', data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6xRAxngRR8o"
      },
      "source": [
        "We can still visualize the images in tensor format, as long as we properly permute the axes and un-normalize them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik3FdO5HRZr_"
      },
      "outputs": [],
      "source": [
        "def visualize_tensor_data(data: torch.Tensor, label: int):\n",
        "    # Data is a tensor of shape [C, W, H]  (C is the channel dimension, 3 for RGB)\n",
        "    # Put channel at last\n",
        "    data = data.permute(1, 2, 0)\n",
        "    # Un-normalize\n",
        "    data = data * torch.as_tensor([0.2470, 0.2435, 0.2616]) + torch.as_tensor([0.4914, 0.4822, 0.4465])\n",
        "    plt.imshow(data)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Label = {class_names[label]}')\n",
        "\n",
        "visualize_tensor_data(data, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZC2GFXHdlT"
      },
      "source": [
        "**Remember, always visualize / play with the data!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ruPHahW2kVq"
      },
      "source": [
        "### Data Loaders\n",
        "For training, we operate on *batched* data. To easily load batched samples and targets, we use `torch.utils.data.DataLoader` and specify the desired batch size and order. For instance,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwGjz2iW2kEB"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(\n",
        "    train_set,       # The dataset to loade\n",
        "    batch_size=512,  # Desired batch size\n",
        "    shuffle=True,    # Whether the batches are random samples of the dataset (without replacement), or sequential segments\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "print('number of batches =', len(loader))\n",
        "\n",
        "# You can easily iterate through it to get data\n",
        "for data, target in loader:\n",
        "    print('*batched* data shape =', data.shape, '  *batched* target shape =', target.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAW_ElprIlZd"
      },
      "source": [
        "## Training a Neural Network\n",
        "\n",
        "**Question 2**: Now, write our training and evaluation functions. Understand the following functions. Follow the comments and fill in the incomplete parts (indicated by `FIXME`), each using `<= 5` lines of code. Add your code to the submission pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyU21dzCIzGS"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def train_epoch(epoch: int, model, train_loader, lr: float):\n",
        "    r\"\"\"\n",
        "    Trains `model` on `train_loader` with learning rate `lr` for one epoch.\n",
        "\n",
        "    Returns the losses computed at each iteration.\n",
        "    \"\"\"\n",
        "    # Loss function to train our classifier\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    loss_values: List[float] = []\n",
        "\n",
        "    # Iterate through the training dataset via `train_loader`\n",
        "    for data, target in tqdm(train_loader, desc=f'Training @ epoch {epoch}'):\n",
        "        # `data` and `target` here are **batched** tensors!\n",
        "        # Convert data and target to proper device for training\n",
        "        data = data.to(device).flatten(1)  # flatten image data into batched vectors\n",
        "        target = target.to(device)\n",
        "        # Clear out any previously computed gradient\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass the model\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(output, target)\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "        # Backpropagate\n",
        "        dLdout = loss_fn.backward(torch.ones((), device=device))  # pass an explicit one because we are computing derivatives w.r.t. the loss itself\n",
        "        model.backward(dLdout)\n",
        "\n",
        "        # Stochastic gradient descent (SGD)\n",
        "        # FIXME\n",
        "        # Here for each `p` a parameter to be optimized, and you want to update it with `p.grad`, the computed `dL/dp`\n",
        "        pass\n",
        "\n",
        "    return loss_values\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    r\"\"\"\n",
        "    Instead of computing losses, we track the true classification accuracy,\n",
        "    where, when the model predicts a distribution over labels, the predicted\n",
        "    label is taken as one with highest probability.\n",
        "    \"\"\"\n",
        "    correct_predcitions = 0\n",
        "    for data, target in loader:\n",
        "        data = data.to(device).flatten(1)  # flatten image data into batched vectors\n",
        "        target = target.to(device)\n",
        "        # Update `correct_predictions`.\n",
        "        # Make sure that you add a *Python number*, not a *PyTorch scalar*.\n",
        "        # Remember that you can use `pytorch_scalar.item()` to get its content\n",
        "        # as a Python number.\n",
        "        # FIXME\n",
        "        pass\n",
        "\n",
        "    acc = correct_predcitions / len(loader.dataset)\n",
        "    return acc\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TrainResult:\n",
        "    r\"\"\"\n",
        "    A collection containing everything we need to know about the training results\n",
        "    \"\"\"\n",
        "\n",
        "    num_epochs: int\n",
        "    lr: float\n",
        "\n",
        "    # The trained model\n",
        "    model: Network\n",
        "\n",
        "    # Training loss (saved at each iteration in `train_epoch`)\n",
        "    train_losses: List[float]\n",
        "\n",
        "    # Training accuracies, before training and after each epoch\n",
        "    train_accs: List[float]\n",
        "\n",
        "    # Validation accuracies, before training and after each epoch\n",
        "    val_accs: List[float]\n",
        "\n",
        "\n",
        "def train(train_set, val_set, *, num_epochs=30, lr=0.15):\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)  # Random order for training (\"[S]tochastic\" in SGD)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=1024, shuffle=False)\n",
        "\n",
        "    # Our classifier\n",
        "    image_tensor_size = train_set[0][0].numel()\n",
        "    model = Network(input_dim=image_tensor_size, device=device)\n",
        "    print('Model =', model)\n",
        "\n",
        "    result: TrainResult = TrainResult(num_epochs, lr, model, train_losses=[], train_accs=[], val_accs=[])\n",
        "    result.train_accs.append(evaluate(model, train_loader))\n",
        "    result.val_accs.append(evaluate(model, val_loader))\n",
        "\n",
        "    # Iterate through the entire training dataset `num_epochs` times\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train over the entire `train_set` with our `train_epoch` function (i.e., one epoch)\n",
        "        result.train_losses.extend(train_epoch(epoch, model, train_loader, lr=lr))\n",
        "        # Evaluate with our `evaluate` function\n",
        "        result.train_accs.append(evaluate(model, train_loader))\n",
        "        result.val_accs.append(evaluate(model, val_loader))\n",
        "        print(f\"Epoch = {epoch:> 2d}    Train loss = {result.train_losses[-1]:.4f}    Train acc = {result.train_accs[-1]:.2%}    Val acc = {result.val_accs[-1]:.2%}\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pgm8jF8BHrG"
      },
      "source": [
        "Now let's perform training! Loss may not seem going down much in the first few epochs, but can decrease much faster later.\n",
        "\n",
        "> NOTE: We are training for 30 epochs, which may take up to 15~20 minutes (including evaluation). Normally, it is not necessary to evaluate on the full validation set after each training epoch. But let's do that for this task, to better see the trends of training and validation accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0trcJGYcN-SK"
      },
      "outputs": [],
      "source": [
        "# This will take a while\n",
        "result = train(train_set, val_set, num_epochs=30, lr=0.15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EONYqkk0bsvG"
      },
      "source": [
        "### Visualize Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2af5DQgeYQsr"
      },
      "outputs": [],
      "source": [
        "def learning_curve(result: TrainResult, *, title: str = 'Learning Curve'):\n",
        "    r\"\"\"\n",
        "    Plot the training loss, training accuracy, and validation accuracy versus\n",
        "    epochs taken.\n",
        "    \"\"\"\n",
        "    fig, ax_loss = plt.subplots(figsize=(8, 5))\n",
        "    ax_loss.set_title(title, fontsize=16)\n",
        "    ax_loss.set_xlabel('Epoch', fontsize=12)\n",
        "\n",
        "    l_trloss = ax_loss.plot(\n",
        "        torch.arange(len(result.train_losses)) / len(result.train_losses) * result.num_epochs,\n",
        "        result.train_losses,\n",
        "        label='Train loss',\n",
        "        color='C0',\n",
        "    )\n",
        "    ax_loss.set_ylim(0, 3)\n",
        "    ax_loss.set_ylabel('Train loss', color='C0', fontsize=12)\n",
        "    ax_loss.tick_params(axis='y', labelcolor='C0')\n",
        "\n",
        "    ax_acc = ax_loss.twinx()\n",
        "    l_tracc = ax_acc.plot(result.train_accs, label='Train acc', color='C1', linestyle='--')\n",
        "    if len(result.val_accs):\n",
        "        l_valacc = ax_acc.plot(result.val_accs, label='Val acc', color='C1')\n",
        "    else:\n",
        "        l_valacc = ()\n",
        "    ax_acc.set_ylim(0, 1)\n",
        "    ax_acc.set_ylabel('Accuracies', color='C1', fontsize=12)\n",
        "    ax_acc.tick_params(axis='y', labelcolor='C1')\n",
        "\n",
        "    lines = l_trloss + l_tracc + l_valacc\n",
        "    ax_loss.legend(lines, [l.get_label() for l in lines], loc='upper left', fontsize=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flr1vM51Bt2q"
      },
      "outputs": [],
      "source": [
        "learning_curve(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P6BDc0rLit4"
      },
      "source": [
        "**Question 3**: Put the above training curve plot in you submission. Comment on any interesting observations.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZpMpqRDWX5x"
      },
      "source": [
        "**Question 4**: Neural networks are universal approximators, which means that we can always find a network that fits the training set. Why do you think that we didn't get perfect training accuracy? Write down some ideas for improving training accuracy. Is a perfect training accuracy all we need?\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JymBuq25L2qu"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "The simplest way to reduce overfitting is to collect more training data. In fact, even with a fixed training set, we can add data augmentations that randomly augment each training image so that we, in a way, increase the effective training dataset size.\n",
        "\n",
        "There are also other ways\n",
        "+ Increase training set size\n",
        "+ Use a more restricted model class (function class) at the cost of not finding as good models\n",
        "+ Regularize the model (e.g., smooth classification boundaries, smoother functions).\n",
        "\n",
        "Here, let's try data augmentations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BAGEQ7sMpDU"
      },
      "source": [
        "Above we used the following function to create the datasets:\n",
        "```py\n",
        "def get_datasets(train_transforms=(), val_transforms=()) -> Tuple[Dataset, Dataset]:\n",
        "    r\"\"\"\n",
        "    Returns the CIFAR-10 training and validation datasets with corresponding\n",
        "    transforms.\n",
        "    \"\"\"\n",
        "```\n",
        "using only the conversion and normalization transforms:\n",
        "```py\n",
        "data_transforms = [\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "**Question 5**: Referencing the [`torchvision.transforms`](https://pytorch.org/vision/0.13/transforms.html) documentation page, create training and validation datasets, where **only the training set** has\n",
        "1. A random cropping augmentation. The cropping should still produce `32x32` images, after padding the image with size `3` (i.e., `3` pixels on each side) with **reflected** borders\n",
        "2. A random horizontal flipping (with probability $0.5$)\n",
        "\n",
        "Please\n",
        "+ Add your code to the submission pdf.\n",
        "+ Visualize the effect of the added augmentation. Plot the `aug_train_set[13]` **multiple times** using the `visualize_tensor_data` function we used above. Attach the images to pdf.\n",
        "\n",
        "You should be able to add the augmentation with two `torchvision.transforms.*` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84mybYWmemgR"
      },
      "outputs": [],
      "source": [
        "# FIXME\n",
        "aug_train_set, val_set = '??'  # get_datasets(?, ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9c2QWX6ggmV"
      },
      "outputs": [],
      "source": [
        "assert tuple(aug_train_set[13][0].shape) == (3, 32, 32)\n",
        "visualize_tensor_data(*aug_train_set[13])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7itAWxEglf-"
      },
      "outputs": [],
      "source": [
        "# A new *randomly* augmented sample. Do this a few times. They should look different than the sample above.\n",
        "visualize_tensor_data(*aug_train_set[13])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brEDO-wbgmPa"
      },
      "outputs": [],
      "source": [
        "visualize_tensor_data(*aug_train_set[13])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_tensor_data(*aug_train_set[13])"
      ],
      "metadata": {
        "id": "qx8IQLzIO5oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_tensor_data(*aug_train_set[13])"
      ],
      "metadata": {
        "id": "BkC0aWONTlhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5zxh1jpOXlC"
      },
      "source": [
        "**Question 6**: Train a new model on the augmented training set. Attach its learning curve and comment on any observed differences from the previous training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJt3b8iWJClO"
      },
      "outputs": [],
      "source": [
        "# Again, this will take a while\n",
        "aug_result = train(aug_train_set, val_set, num_epochs=30, lr=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbYzGzHPevVp"
      },
      "outputs": [],
      "source": [
        "learning_curve(aug_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}